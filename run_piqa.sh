export CUDA_VISIBLE_DEVICES=1
python mola_training.py \
         --base_model "/home/sub6-zt/caonan/model/Llama-3.1-8B" \
         --data_path "/home/sub6-zt/caonan/dataset/piqa_dataset_dict" \
         --output_dir "./results/qa_piqa_mola-3ep-1" \
         --batch_size 16 \
         --micro_batch_size 2 \
         --num_epochs 3 \
         --learning_rate 5e-5 \
         --cutoff_len 256 \
         --val_set_size 1 \
         --lora_r "8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8" \
         --lora_alpha 16 \
         --lora_dropout 0.075 \
         --lora_target_modules "q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj" \
         --number_experts "2,2,2,2,2,2,2,2,4,4,4,4,4,4,4,4,6,6,6,6,6,6,6,6,8,8,8,8,8,8,8,8" \
         --top_k "2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2" \
         --train_on_inputs \
         --group_by_length \
         --add_eos_token \
         --obalance true